{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2260ac5-acfa-4d52-b71a-3e58bc13807b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7169fc7b520648728e0ebe9f109743e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resized embeddings to 256002\n",
      "✅ merged model written to /workspace/Gemma-2-2b-it-ChatDoctor-MedQA/gemma2-2b-chatdoctor-merged\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch, os, json\n",
    "\n",
    "# ── paths ──────────────────────────────────────────────────────────────\n",
    "BASE_NAME   = \"google/gemma-2-2b-it\"              # full checkpoint on HF Hub\n",
    "ADAPTER_DIR = \"/workspace/Gemma-2-2b-it-ChatDoctor-MedQA\"  # where adapter_config.json lives\n",
    "OUT_DIR     = \"gemma2-2b-chatdoctor-merged\"       # <- will be created\n",
    "\n",
    "# ── 1. load base model & tokenizer (FULL precision) ────────────────────\n",
    "tok = AutoTokenizer.from_pretrained(BASE_NAME, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_NAME,\n",
    "    torch_dtype=torch.float16,        # or bfloat16 if your GPU supports it\n",
    "    device_map={\"\": 0},               # GPU‑0; use \"cpu\" if no GPU (needs ~12 GB RAM)\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# ── 2. make the vocab 256 002 tokens long ──────────────────────────────\n",
    "if model.get_input_embeddings().num_embeddings == 256_000:\n",
    "    # add two placeholder tokens; names don't really matter\n",
    "    extra = [\"<extra_0>\", \"<extra_1>\"]\n",
    "    tok.add_tokens(extra, special_tokens=True)\n",
    "    model.resize_token_embeddings(len(tok))       # now 256002 × 2304\n",
    "    print(\"Resized embeddings to\", len(tok))\n",
    "\n",
    "# ── 3. attach LoRA adapter and merge ───────────────────────────────────\n",
    "model = PeftModel.from_pretrained(model, ADAPTER_DIR)\n",
    "model = model.merge_and_unload()                  # succeeds because sizes now match\n",
    "\n",
    "# ── 4. save the merged checkpoint ──────────────────────────────────────\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "model.save_pretrained(OUT_DIR, safe_serialization=True)\n",
    "tok.save_pretrained(OUT_DIR)\n",
    "\n",
    "print(\"✅ merged model written to\", os.path.abspath(OUT_DIR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e48f2e3e-2d3a-40ee-84eb-cb5f573804f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9c7d3b2d4854cadbd0ac26ff59ca76a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!pip install -U huggingface_hub git-lfs   # git‑lfs must also be on PATH\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Hugging Face\n",
    "login()                       # auto‑detects HF_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "018db1e2-c7f5-437a-86f3-ba9949186bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found merged model at: /workspace/Gemma-2-2b-it-ChatDoctor-MedQA/gemma2-2b-chatdoctor-merged\n",
      "Files in merged model directory:\n",
      "  - tokenizer.json\n",
      "  - special_tokens_map.json\n",
      "  - tokenizer_config.json\n",
      "  - chat_template.jinja\n",
      "  - model.safetensors.index.json\n",
      "  - model-00002-of-00002.safetensors\n",
      "  - model-00001-of-00002.safetensors\n",
      "  - generation_config.json\n",
      "  - config.json\n",
      "Loading merged model for upload...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "841665bf6f1f4c9481f8dc943528df16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading model to Cshavi/gemma2-2b-chatdoctor-medqa_merged...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad70be90cb2d4507a6c396a946640543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "800e694b4d2b42539bf2711b1cb9149f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "099aa4ce153640e5959653e73a8da052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/241M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4261a36b85934ec0bbabecf48eabeba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "964d8e672aef40ebaa6568a2b732a38e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/34.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model uploaded successfully to: https://huggingface.co/Cshavi/gemma2-2b-chatdoctor-medqa_merged\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Your merged model is already saved in this directory\n",
    "merged_model_path = \"gemma2-2b-chatdoctor-merged\"\n",
    "\n",
    "# Verify the merged model exists\n",
    "if os.path.exists(merged_model_path):\n",
    "    print(f\"✅ Found merged model at: {os.path.abspath(merged_model_path)}\")\n",
    "    \n",
    "    # List files in the directory\n",
    "    files = os.listdir(merged_model_path)\n",
    "    print(\"Files in merged model directory:\")\n",
    "    for file in files:\n",
    "        print(f\"  - {file}\")\n",
    "else:\n",
    "    print(f\"❌ Merged model directory not found: {merged_model_path}\")\n",
    "    exit()\n",
    "\n",
    "# Login to Hugging Face (if not already logged in)\n",
    "# login()  # Uncomment if you need to login\n",
    "\n",
    "# Load the merged model and tokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(\"Loading merged model for upload...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    merged_model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(merged_model_path, trust_remote_code=True)\n",
    "\n",
    "# Upload to Hugging Face\n",
    "repo_name = \"Cshavi/gemma2-2b-chatdoctor-medqa_merged\"\n",
    "\n",
    "print(f\"Uploading model to {repo_name}...\")\n",
    "model.push_to_hub(repo_name, use_temp_dir=False)\n",
    "tokenizer.push_to_hub(repo_name, use_temp_dir=False)\n",
    "\n",
    "print(f\"✅ Model uploaded successfully to: https://huggingface.co/{repo_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d82b6c6-4a7a-4763-ba17-5a857920eef0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
