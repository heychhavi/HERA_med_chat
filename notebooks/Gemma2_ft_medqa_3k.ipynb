{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f62481b7-48fe-484c-a178-2dd30c731be7",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "751fd194-df9d-42df-9622-0c94ce584ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.7.0 which is incompatible.\n",
      "torchvision 0.16.0+cu118 requires torch==2.1.0, but you have torch 2.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q accelerate peft transformers bitsandbytes datasets\n",
    "!pip install -q wandb\n",
    "!pip install -q tqdm\n",
    "!pip install -q trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cbcbe9b-f1b9-4c3c-b43f-d52e42a7082d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install flash-attn --no-build-isolation --find-links https://github.com/Dao-AILab/flash-attention/releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77519332-73fd-4920-9dd7-7bda21176512",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install flash-attn --no-deps --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ca58af4-15b0-4ece-8bab-098d20651fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.10/dist-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    BitsAndBytesConfig, \n",
    "    TrainingArguments, \n",
    "    logging\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "#from kaggle_secrets import UserSecretsClient\n",
    "from huggingface_hub import login\n",
    "from trl import SFTTrainer, setup_chat_format\n",
    "import bitsandbytes as bnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9efe0328-5a96-4e97-94dc-0f8306cc9179",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'kaggle_secrets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkaggle_secrets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UserSecretsClient\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'kaggle_secrets'"
     ]
    }
   ],
   "source": [
    "from kaggle_secrets import UserSecretsClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d495d09-1d28-436e-b892-5cf43756a867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TRANSFORMERS_OFFLINE'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7062fbcb-4cb5-4df4-a1ae-5cac1f1a680e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one‑off in your shell; in CI put them in the secret store instead\n",
    "!export HF_TOKEN='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'\n",
    "!export WANDB_API_KEY='860115c5021e6fc41d21047033c72bdd1bac4410'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0df497e3-e29b-4e62-96cb-c45b5097e92f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f95a960a76045c29e33a486f979dfde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchhavin6v\u001b[0m (\u001b[33moutlier89\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20250531_181029-oastlh2t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/outlier89/Fine%E2%80%91tune%20Gemma%E2%80%912%E2%80%912b%E2%80%91it%20on%20Medical%20QA/runs/oastlh2t?apiKey=860115c5021e6fc41d21047033c72bdd1bac4410' target=\"_blank\">sunny-sea-2</a></strong> to <a href='https://wandb.ai/outlier89/Fine%E2%80%91tune%20Gemma%E2%80%912%E2%80%912b%E2%80%91it%20on%20Medical%20QA?apiKey=860115c5021e6fc41d21047033c72bdd1bac4410' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/outlier89/Fine%E2%80%91tune%20Gemma%E2%80%912%E2%80%912b%E2%80%91it%20on%20Medical%20QA?apiKey=860115c5021e6fc41d21047033c72bdd1bac4410' target=\"_blank\">https://wandb.ai/outlier89/Fine%E2%80%91tune%20Gemma%E2%80%912%E2%80%912b%E2%80%91it%20on%20Medical%20QA?apiKey=860115c5021e6fc41d21047033c72bdd1bac4410</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/outlier89/Fine%E2%80%91tune%20Gemma%E2%80%912%E2%80%912b%E2%80%91it%20on%20Medical%20QA/runs/oastlh2t?apiKey=860115c5021e6fc41d21047033c72bdd1bac4410' target=\"_blank\">https://wandb.ai/outlier89/Fine%E2%80%91tune%20Gemma%E2%80%912%E2%80%912b%E2%80%91it%20on%20Medical%20QA/runs/oastlh2t?apiKey=860115c5021e6fc41d21047033c72bdd1bac4410</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Do NOT share these links with anyone. They can be used to claim your runs."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os, wandb\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Hugging Face\n",
    "login()                       # auto‑detects HF_TOKEN\n",
    "\n",
    "# W&B\n",
    "wandb.login()                 # auto‑detects WANDB_API_KEY\n",
    "run = wandb.init(\n",
    "    project=\"Fine‑tune Gemma‑2‑2b‑it on Medical QA\",\n",
    "    job_type=\"training\",\n",
    "    anonymous=\"allow\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9380fc75-2882-40ba-b850-d3f2cba7a781",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2a6b291-3e9a-4ccb-ac11-7d71e444f7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"google/gemma-2-2b-it\"\n",
    "new_model = \"Gemma-2-2b-it-ChatDoctor-MedQA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a301991-b265-455b-af01-f7d7dfefb5af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "259aa435439e400b8dca8c414fb37fa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    }
   ],
   "source": [
    "# Adjust precision and attention based on GPU\n",
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    torch_dtype = torch.bfloat16\n",
    "    attn_implementation = \"sdpa\"  # Use PyTorch's scaled dot product attention\n",
    "else:\n",
    "    torch_dtype = torch.float16\n",
    "    attn_implementation = \"eager\"  # Standard attention implementation\n",
    "\n",
    "# BitsAndBytes configuration for memory-efficient model loading\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load model with quantization and optimized attention\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=attn_implementation,\n",
    "    torch_dtype=torch_dtype  # Explicitly set dtype\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "\n",
    "# Efficient LoRA fine-tuning configuration\n",
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    lora_module_names.discard('lm_head')  # Exclude lm_head for 16-bit\n",
    "    return list(lora_module_names)\n",
    "\n",
    "modules = find_all_linear_names(model)\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=modules\n",
    ")\n",
    "tokenizer.chat_template = None\n",
    "\n",
    "model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dab3bd9-3de9-4955-beb2-675a1ad2eefc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "883050bc649b46a6af9896742873e04a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db7dc236e9474cb7978f6addbc5b88e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/233M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a4ab6bdb24d4c2d8eb8523b46f4051d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/211269 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb45eea00a204fcfaf25a9978128ca9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load PubMedQA dataset\n",
    "dataset = load_dataset(\"qiaojin/PubMedQA\", \"pqa_artificial\", split=\"train\", cache_dir=\"./cache\")\n",
    "dataset = dataset.shuffle(seed=42).select(range(3000))  # Use 3k samples for a better demo\n",
    "\n",
    "def format_chat_template(row):\n",
    "    # Extract context from the JSON structure\n",
    "    contexts = row[\"context\"][\"contexts\"]\n",
    "    # Join all contexts into a single context string\n",
    "    context_text = \" \".join(contexts)\n",
    "    \n",
    "    # Combine long_answer and final_decision\n",
    "    assistant_response = f\"{row['long_answer']} {row['final_decision']}\"\n",
    "    \n",
    "    # Create the chat template\n",
    "    row_json = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a medical expert. Answer the question based on the provided context.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Context: {context_text}\\n\\nQuestion: {row['question']}\"},\n",
    "        {\"role\": \"assistant\", \"content\": assistant_response}\n",
    "    ]\n",
    "    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n",
    "    return row\n",
    "\n",
    "dataset = dataset.map(format_chat_template, num_proc=4)\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "# Dynamic padding for efficiency\n",
    "data_collator = lambda batch: tokenizer(batch[\"text\"], return_tensors=\"pt\", padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a66e730e-c64b-4843-bfbf-489bd0e0e2c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0708fdded4f4948ab7cf2d18952495f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting train dataset to ChatML:   0%|          | 0/2700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85b8471e9c1a4efeb3b7fd1c29f61272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/2700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a3866eab5c543b29ed98d17909b2bad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/2700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "236bbdf729f846439d67def82db09447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/2700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69da67cd46c94772b67f5f228377417b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting eval dataset to ChatML:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef7ca3dbd1ad48439939a27067e2d659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to eval dataset:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39307a816e5c417ca1074c7938370b44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccd26d0fd75f4cfc90ed9773631ecea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=new_model,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=1,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,  \n",
    "    save_steps=500,  \n",
    "    logging_steps=1,\n",
    "    warmup_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=2e-4,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    group_by_length=True,\n",
    "    report_to=\"wandb\",\n",
    "    load_best_model_at_end=False\n",
    ")\n",
    "\n",
    "# Simplified SFTTrainer - minimal parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "\n",
    "# Disable caching during training for gradient computation efficiency\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a02a361c-e50f-4153-b419-beff3c304e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `sdpa`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1350' max='1350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1350/1350 14:13, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.861000</td>\n",
       "      <td>1.558751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.724700</td>\n",
       "      <td>1.546588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.403200</td>\n",
       "      <td>1.540742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.384400</td>\n",
       "      <td>1.531562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.640800</td>\n",
       "      <td>1.524979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.704000</td>\n",
       "      <td>1.519578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1350, training_loss=1.5504572401664876, metrics={'train_runtime': 854.5098, 'train_samples_per_second': 3.16, 'train_steps_per_second': 1.58, 'total_flos': 1.5011354108708352e+16, 'train_loss': 1.5504572401664876})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18d8ee72-049b-4ecf-bb9b-6ea1f0e326c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▆▅▃▂▁</td></tr><tr><td>eval/mean_token_accuracy</td><td>▁▃▄▆▇█</td></tr><tr><td>eval/num_tokens</td><td>▁▂▄▅▇█</td></tr><tr><td>eval/runtime</td><td>▄▂█▆▃▁</td></tr><tr><td>eval/samples_per_second</td><td>▄▇▁▃▆█</td></tr><tr><td>eval/steps_per_second</td><td>▄▇▁▃▆█</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇█████</td></tr><tr><td>train/grad_norm</td><td>▆▃▅▅▆▄▃▄▃▆█▁▄▅▅▃▅▃▃▄▄▅▃▇▄▄▃▄▄▄▄▃▃▄▃▄▄▅▁▂</td></tr><tr><td>train/learning_rate</td><td>██████▇▇▇▇▇▇▆▆▆▅▅▅▅▅▅▅▅▅▅▄▄▄▄▃▂▂▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>▃▆█▃▄▁▄▆▃▇▃▃▃▄▅▄█▅▆▄▇▄▄▅▄▅▆▄▆▃▇▃▆▅▄▅▃▄▃▂</td></tr><tr><td>train/mean_token_accuracy</td><td>▂▄▅▅▁▄▅█▇▅▅▄▄▃▅▄▄▇▃▅▂▆▄▃▄▃▆▅▂▅▄▅▆█▃▆▄▇▆▅</td></tr><tr><td>train/num_tokens</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.51958</td></tr><tr><td>eval/mean_token_accuracy</td><td>0.6419</td></tr><tr><td>eval/num_tokens</td><td>1089037.0</td></tr><tr><td>eval/runtime</td><td>27.4116</td></tr><tr><td>eval/samples_per_second</td><td>10.944</td></tr><tr><td>eval/steps_per_second</td><td>10.944</td></tr><tr><td>total_flos</td><td>1.5011354108708352e+16</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>1350</td></tr><tr><td>train/grad_norm</td><td>1.65964</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.4721</td></tr><tr><td>train/mean_token_accuracy</td><td>0.66853</td></tr><tr><td>train/num_tokens</td><td>1223249.0</td></tr><tr><td>train_loss</td><td>1.55046</td></tr><tr><td>train_runtime</td><td>854.5098</td></tr><tr><td>train_samples_per_second</td><td>3.16</td></tr><tr><td>train_steps_per_second</td><td>1.58</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sunny-sea-2</strong> at: <a href='https://wandb.ai/outlier89/Fine%E2%80%91tune%20Gemma%E2%80%912%E2%80%912b%E2%80%91it%20on%20Medical%20QA/runs/oastlh2t?apiKey=860115c5021e6fc41d21047033c72bdd1bac4410' target=\"_blank\">https://wandb.ai/outlier89/Fine%E2%80%91tune%20Gemma%E2%80%912%E2%80%912b%E2%80%91it%20on%20Medical%20QA/runs/oastlh2t?apiKey=860115c5021e6fc41d21047033c72bdd1bac4410</a><br> View project at: <a href='https://wandb.ai/outlier89/Fine%E2%80%91tune%20Gemma%E2%80%912%E2%80%912b%E2%80%91it%20on%20Medical%20QA?apiKey=860115c5021e6fc41d21047033c72bdd1bac4410' target=\"_blank\">https://wandb.ai/outlier89/Fine%E2%80%91tune%20Gemma%E2%80%912%E2%80%912b%E2%80%91it%20on%20Medical%20QA?apiKey=860115c5021e6fc41d21047033c72bdd1bac4410</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250531_181029-oastlh2t/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()\n",
    "model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5dcd342e-80cd-4b2e-a9f6-2a36e97a441d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d11c2edfecc45809ad468a6163c5bab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/2.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Cshavi/Gemma-2-2b-it-ChatDoctor-MedQA/commit/db7ac656557744901eb4e29803a1978ed3572630', commit_message='Upload model', commit_description='', oid='db7ac656557744901eb4e29803a1978ed3572630', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Cshavi/Gemma-2-2b-it-ChatDoctor-MedQA', endpoint='https://huggingface.co', repo_type='model', repo_id='Cshavi/Gemma-2-2b-it-ChatDoctor-MedQA'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.save_pretrained(new_model)\n",
    "trainer.model.push_to_hub(new_model, use_temp_dir=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5c703e4-030f-4ec4-94e6-6edbc93c7c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I understand you are looking for advice on how to get rid off acne. I am an AI and cannot give medical advice. It is important to see a dermatologist to get a proper diagnosis and treatment plan. However, I can give you some general information about acne.\n",
      "\n",
      "Acne is a common skin condition that occurs when hair follicles become clogged with oil and dead skin cells. The clogged follicles can become inflamed, which causes the characteristic red, white, or black bumps that are associated with acne. Acne can occur on the face, chest, back, and shoulders.\n",
      "\n",
      "There are many different types of acne, and the best treatment for acne will vary depending on the type of acne and the severity of the acne. Some of the most common types of treatment for mild acne include:\n",
      "\n",
      "* **Over-the-counter (OTC) products:** OTC products are available over the counter at most drugstores. OTC products can help to reduce the appearance of acne by removing dead skin and oil from the skin. OTC acne products include benzoyl peroxide, salicylic acid, and retinoids.\n",
      "* **Prescription products:** Prescription products are prescribed by a dermatologist. Prescription products can be more effective than OTC products, and they can be used to treat more severe acne. Prescription acne products can include retinoids, antibiotics, and anti-inflammatory agents.\n",
      "\n",
      "If you are concerned about acne, it is important that you see a doctor. A doctor can help you to determine the best course of treatment.\n",
      "\n",
      "Here are some additional tips for managing acne:\n",
      "\n",
      "- Wash your face twice a day with a mild\n"
     ]
    }
   ],
   "source": [
    "messages = [{\"role\": \"user\", \"content\": \"Hello doctor, I have bad and painfull acne on face and body. How can I get rid of it?\"}]\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
    "\n",
    "# Optimized generation with tuned sampling strategies\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_length=350,  # Increase max length for complex answers\n",
    "    num_return_sequences=1,\n",
    "    top_k=50,\n",
    "    top_p=0.85,  # Narrow top-p for more deterministic output\n",
    "    temperature=0.3,  # Slightly higher temperature for balance between creativity and accuracy\n",
    "    no_repeat_ngram_size=3,\n",
    ")\n",
    "\n",
    "# Decode and clean up the output\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "response = text.split(\"assistant\")[1].strip()\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5416fde-e58b-4ce2-9719-eeced9cfc6c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
